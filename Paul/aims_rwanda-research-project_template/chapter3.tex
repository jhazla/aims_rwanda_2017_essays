\chapter{Methodology}
\section{Unsupervised Learning (SL)}
This is a machine learning task of inferring a function to describe hidden structure from unlabelled data. This type of ML does not require any prior manual categorization of observations in the data. 

The distinction between supervised learning and unsupervised learning (UL) is that in unsupervised learning there is no evaluation of accuracy of the algorithm used, because data fed to the learner is unlabelled . Also one advantage of UL over SL is that time and cost is saved in labelling as required in SL. 
\section{Natural Language Processing (NLP)}
It is a multidisciplinary area that deals with the automatic processing of human language. 
This automation allows communication between humans and computers. The computer accept input in the form of text or speech and then produces structured representations showing the meaning of those strings as their output.
\section{Data}
The source of the data for this research is from the website of IFRC. Practically the data was obtained by algorithms implemented in the R studio, automatically downloaded the over one thousand pdf reports from the website. Each report is named a name of a country depicting that the reports describes disaster that occurred in a particular country. Each report has an appeal id, several documents might refer to the same appeal id. The appeal id is the unique code given a particular report. Reports describing the same event have the share the same appeal id.
%\Jnote{Explain what is appeal and what is appeal id.}
\section{Gensim}
Gensim is an open source toolkit implemented in python to execute task involving vector space models and topic modelling \cite{rehurek2010software}. Some features of Gensim employed in this research are "term frequency inverse document frequency (TFIDF)" and "LDA", "LSA". Before executing the above features the "corpora" and "doc2bow" modules are used to represent large collection of texts and to convert the text collection into vectors respectively.
\section{Natural  Language Toolkit (NLTK)}
This is also an open library with set of modules that enhances the processing of human language. It is originally developed by Steven Bird and Edward Loper both in  the Department of Computer and Information Science at the University of Pennsylvania. This provided a landmark for researchers to contribute to making it more robust and an efficient library. The "corpus" and the "tokenize" are some modules relevant in topic modelling.
\section{Word Embeddings}
Word embeddings is a dense representation of words in a low dimensional vector space. Bigo et al(2003) introduced the  concept of word embedding and then train them in neural language jointly with model parameters. Mikolov et al (2013)came out with the popular word embedding model known as the Word2vec. 
Pemigton et al (2014) released Glove. The Glove and the Wor2vec are both aimed at producing word embeddings that ecode the general semantic relationship.
\section{Tokenization}
This describes the process of splitting  a text or a collection of texts  into each single term constituting the text. Each term is known as the token. It can be a "word", "symbols", "punctuations", "numbers". For example given text "she won a prize worth 30 million dollars", after tokenizing we have "she", "won", "a", "prize", "worth", "30", "million", "dollars". Prior to creating a vector representation of terms in the document tokenizing is done. The Natural Language Toolkit (NLTK) library is employed to implement this process. 
\section{Term Frequency Inverse Document Frequency (TFID)}
This measures the extent to which words are important in a document. In topic modelling we want to find a group of words that describes a vocabulary. For example topic modelling a document that talks about a university, words such as "classrooms", "library", "lecturs", "Courses", "Grades" would tend to be the most important words that describe the topic. It is worth noting that important words are not necessarily the most frequent words, possible to be judged by our intuitive notions. 

The TFIDF transforms a vector of integer values into a vector of real values, maintaining the dimension of the original vector.After transformation  features which are not frequent in the corpus will have their values increased. That does not mean that  all rare words important, some may not be significant at all in the description of the topic.
%\Jnote{Rare words all rare words?}
For instance dealing with our "university" document, a word like "congregation" may rare but then it is significant towards describing the vocabulary. On the other hand a word such as "consequently" may appear very frequent which in this case does not really say anything about the topic. The most frequent words are
most words such as “the” or “and,” which helps to construct a sentence, thereby making it readable and understandable.These words do not carry any importance to help topic model a document.They are stop words and they are removed before the modelling irrespective of their number.

 Given a collection of document with each document $d$ containing words, where each word in the document is denoted $i$. The frequency of occurrence of a word $i$ in document $d$ is denoted $f_{id}$. The term frequency $TF_{id}$ computed as:
 
$$TF_{id}=\frac{f_{id}}{\text{max}_tf_{td}}$$.
%\Jnote{What is $j$?}
Which means that the  frequency of the word  i in document d is $f_{id }$normalized by dividing
it by the term with the highest frequency in the same document of occurrence with stop words exclusive.
%\Jnote{You forgot math mode.}
Intuitively the word which occurs most frequently would have  would have a $TF$ of 1,
%\Jnote{word which occurs what?}
and other words get fractions as their term frequency for this document.
%Theorems before the chapter's first section will be dot-zero, 
%and their numbering is completely wrong. You can avoid this
%by simply always starting a chapter with a section. Ta Da! 
%It will probably help you structure your essay anyway. 
%
%\begin{thm}[My Theorem2]
%This is my theorem2.
%\end{thm}
%\begin{proof}
%And it has no proof2.
%\end{proof}
%
%\section{See?}
%
%Text text text text text text text text text text text text text text
%text text text text text text text text text text text text text text
%text text text text text text text text text text text text text text
%text text text text text text text text text text text text text text
%text text text text text.
%
%\begin{thm}[My Theorem2]
%This is my theorem2.
%\end{thm}
%\begin{proof}
%And it has no proof2.
%\end{proof}
%
%Text text text text text text text text text text text text text text
%text text text text text text text text text text text text text text
%text text text text text text text text text text text text text text
%text text text text text text text text text text text text text text
%text text text text text.
%
%\begin{align} % do not use eqnarray. 
%\label{2ya}
%x & = y + y\\
%\label{2yb}
%& = 2y
%\end{align}
%see equations \eqref{2ya} and \ref{2yb}
%
%\section{More}
%
%Here's a conjecture
%\begin{conj}
%The washing operation has fixed points.
%\end{conj}
%
%and here's an example
%
%\begin{exa}
%100 FRW coin.
%\end{exa}
%
%\subsection{This is a subsection}
%
%\section{This is a section}
